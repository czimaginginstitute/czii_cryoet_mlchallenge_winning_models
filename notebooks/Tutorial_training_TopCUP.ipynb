{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT5SZxmxZLZE"
   },
   "source": [
    "#  Train TopCUP models to extract protein particels in cryoET dataset\n",
    "\n",
    "**Estimated time to complete:** 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbeoPlg5aCnw"
   },
   "source": [
    "## Learning Goals\n",
    "* Create a copick configuration file for loading cryoET dataset.\n",
    "* Train TopCUP models and automatically save best checkpoints via its command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb7T-ymlaOKy"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "The TopCUP model requires `python>=3.10`. Install the environment using the command: `pip install git+https://github.com/czimaginginstitute/czii_cryoet_mlchallenge_winning_models.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqgtQsXOayO7"
   },
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "The Top CryoET U-Net Picker (TopCUP) is a 3D U-Net–based ensemble model designed for particle picking in cryo-electron tomography (cryoET) volumes.\n",
    "It uses a segmentation heatmap approach to identify particle locations.\n",
    "TopCUP is fully integrated with copick — a flexible cryoET dataset API developed at the Chan Zuckerberg Imaging Institute (CZII).\n",
    "This integration makes it easy to apply the model directly to any cryoET dataset in copick format.\n",
    "The only input required is a copick configuration file.\n",
    "\n",
    "The copick configuration file must define **pickable objects** (i.e., the protein complexes you want to detect) and **three** key metadata parameters for each object:\n",
    "* score weight: weight for each class in the DenseCrossEntropy loss\n",
    "* score_threshold: threshold to filter final picks per class, reducing false positives\n",
    "* score_weight: weight for each class in the F-beta score evaluation\n",
    "\n",
    "\n",
    "For this tutorial, we’ll use seven tomograms from the Experimental Training Dataset (Dataset ID: DS-10440)--the same dataset used in the Kaggle CryoET Challenge.\n",
    "Now that this dataset is publicly available on the CZ CryoET Data Portal,\n",
    "we can stream it directly using the copick configuration file provided below.\n",
    "We can automatically generate a copick configuration file from cryoET dataportal, and add metadata for each particles for training TopCUP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copick\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    \"ferritin-complex\": {\n",
    "        \"score_weight\": 1,\n",
    "        \"score_threshold\": 0.16,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"thyroglobulin\": {\n",
    "        \"score_weight\": 2,\n",
    "        \"score_threshold\": 0.18,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"beta-galactosidase\": {\n",
    "        \"score_weight\": 2,\n",
    "        \"score_threshold\": 0.13,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"beta-amylase\": {\n",
    "        \"score_weight\": 0,\n",
    "        \"score_threshold\": 0.25,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"cytosolic-ribosome\": {\n",
    "        \"score_weight\": 1,\n",
    "        \"score_threshold\": 0.19,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"virus-like-capsid\": {\n",
    "        \"score_weight\": 1,\n",
    "        \"score_threshold\": 0.5,\n",
    "        \"class_loss_weight\": 256\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "copick_config_path = os.path.abspath('./training_copick_config_portal.json')\n",
    "overlay_path = os.path.abspath('./tmp_overlay')\n",
    "copick_root = copick.from_czcdp_datasets(\n",
    "    [10440], #dataset_ids\n",
    "    overlay_path,  \n",
    "    {'auto_mkdir': True}, #overlay_root, self-defined\n",
    "    output_path = copick_config_path,\n",
    ")\n",
    "\n",
    "# only consider the 6 particles\n",
    "config_pickable_objects = []\n",
    "for p in copick_root.config.pickable_objects:\n",
    "    if p.name in metadata:\n",
    "        p.metadata = metadata[p.name]\n",
    "        config_pickable_objects.append(p)\n",
    "\n",
    "copick_root.config.pickable_objects = config_pickable_objects\n",
    "# save the copick config for later use\n",
    "copick_root.save_config(copick_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Resources\n",
    "\n",
    "You can find additional instructions and template configurations for accessing datasets across different platforms in the official [copick document](https://copick.github.io/copick/examples/overview/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChKuhOlbbzX2"
   },
   "source": [
    "## Use Case\n",
    "\n",
    "To explore the available options for running TopCUP, use the --help flag. In your terminal, run `topcup train --help`. This will display all command-line options and arguments for running TopCUP training, see below:\n",
    "\n",
    "```\n",
    "Usage: topcup train [OPTIONS]\n",
    "\n",
    "Options:\n",
    "  -c, --copick_config FILE      copick config file path  [required]\n",
    "  -tts, --train_run_names TEXT  Tomogram dataset run names for training\n",
    "                                [required]\n",
    "  -vts, --val_run_names TEXT    Tomogram dataset run names for validation\n",
    "                                [required]\n",
    "  -tt, --tomo_type TEXT         Tomogram type. Default is denoised.\n",
    "  -u, --user_id TEXT            Needed for training, the user_id used for the\n",
    "                                ground truth picks.\n",
    "  -s, --session_id TEXT         Needed for training, the session_id used for\n",
    "                                the ground truth picks. Default is None.\n",
    "  -bs, --batch_size INTEGER     batch size for data loader\n",
    "  -n, --n_aug INTEGER           Data augmentation copy. Default is 1112.\n",
    "  -l, --learning_rate FLOAT     Learning rate for optimizer\n",
    "  -p, --pretrained_weight TEXT  One pretrained weights file path. Default is\n",
    "                                None.\n",
    "  -e, --epochs INTEGER          Number of epochs. Default is 100.\n",
    "  --pixelsize FLOAT             Pixelsize in angstrom. Default is 10.0A.\n",
    "  -o, --output_dir TEXT         output dir for saving checkpoints\n",
    "  -v, --logger_version INTEGER  PyTorch-Lightning logger version. If not set,\n",
    "                                logs and outputs will increment to the next\n",
    "                                version.\n",
    "  -h, --help                    Show this message and exit.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Options\n",
    "You can explore dataset-specific options such as `run_names`, `pixelsize`, `tomo_type`, and annotator `user_id` using the copick API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run name: 16463, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 16464, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 16465, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 16466, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 16467, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 16468, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 16469, annotation user_id: data-portal, available voxelsize/pixelsize: 4.99,10.012 A\n"
     ]
    }
   ],
   "source": [
    "# Check available run names\n",
    "for run in copick_root.runs:\n",
    "    pss = [str(vs.voxel_size) for vs in run.voxel_spacings]\n",
    "    ps = ','.join(set(pss))\n",
    "    users = [p.user_id for p in run.picks]\n",
    "    urs = ','.join(set(users))\n",
    "    print(f\"run name: {run.name}, annotation user_id: {urs}, available voxelsize/pixelsize: {ps} A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 16463 has tomogram_type: wbp-denoised-denoiset-ctfdeconv,wbp-filtered-ctfdeconv\n"
     ]
    }
   ],
   "source": [
    "# Get a single run\n",
    "run = copick_root.get_run('16463')\n",
    "voxel_spacing_obj = run.get_voxel_spacing(10.012)\n",
    "\n",
    "# Check available reconstruction_type\n",
    "tts = [t.tomo_type for t in voxel_spacing_obj.tomograms]\n",
    "tt = ','.join(tts)\n",
    "print(f'run {run.name} has tomogram_type: {tt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CaTBGVOSehLb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger version 0\n",
      "logger log_dir /hpc/projects/group.czii/kevin.zhao/ml_challenge/winning_models/czii_cryoet_mlchallenge_models/outputs_training/logs/training_logs/version_0\n",
      "making output dir /hpc/projects/group.czii/kevin.zhao/ml_challenge/winning_models/czii_cryoet_mlchallenge_models/outputs_training/jobs/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /hpc/projects/group.czii/kevin.zhao/ml_challenge/winning_models/czii_cryoet_mlchallenge_models/outputs_training/logs/training_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /hpc/projects/group.czii/kevin.zhao/ml_challenge/winning_models/czii_cryoet_mlchallenge_models/outputs_training/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint dir: /hpc/projects/group.czii/kevin.zhao/ml_challenge/winning_models/czii_cryoet_mlchallenge_models/outputs_training/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type              | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model   | FlexibleUNet      | 70.7 M | train\n",
      "1 | loss_fn | DenseCrossEntropy | 0      | train\n",
      "2 | mixup   | Mixup             | 0      | train\n",
      "------------------------------------------------------\n",
      "70.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "70.7 M    Total params\n",
      "282.647   Total estimated model params size (MB)\n",
      "212       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_dataset length: 1\n",
      "Epoch 0: 100%|██████████| 12/12 [06:49<00:00,  0.03it/s, v_num=0, train_loss_step=1.490]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[APredicting TS 16469\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:05<00:00,  0.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score threshold values {'cytosolic-ribosome': 0.19, 'virus-like-capsid': 0.5, 'beta-galactosidase': 0.13, 'ferritin-complex': 0.16, 'beta-amylase': 0.25, 'thyroglobulin': 0.18}\n",
      "{'score_cytosolic-ribosome': 0.06685393258426967, 'score_virus-like-capsid': 0.0, 'score_beta-galactosidase': 0.0005392802195187717, 'score_ferritin-complex': 0.0, 'score_beta-amylase': 0.0, 'score_thyroglobulin': 0.00847457627118644, 'score': np.float64(0.012125949366525727)}\n",
      "\n",
      "Epoch 1: 100%|██████████| 12/12 [06:55<00:00,  0.03it/s, v_num=0, train_loss_step=1.060, val_loss_step=0.000, val_loss_epoch=0.000, val_score=0.0121, train_loss_epoch=1.770]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[APredicting TS 16469\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:04<00:00,  0.21it/s]\u001b[ABest score threshold values {'cytosolic-ribosome': 0.19, 'virus-like-capsid': 0.5, 'beta-galactosidase': 0.13, 'ferritin-complex': 0.16, 'beta-amylase': 0.25, 'thyroglobulin': 0.18}\n",
      "{'score_cytosolic-ribosome': 0.037465564738292004, 'score_virus-like-capsid': 0.0, 'score_beta-galactosidase': 0.00035769141751004703, 'score_ferritin-complex': 0.1259259259259259, 'score_beta-amylase': 0.0, 'score_thyroglobulin': 0.00794268805482012, 'score': np.float64(0.02571317851555404)}\n",
      "\n",
      "Epoch 2: 100%|██████████| 12/12 [06:52<00:00,  0.03it/s, v_num=0, train_loss_step=0.720, val_loss_step=0.000, val_loss_epoch=0.000, val_score=0.0257, train_loss_epoch=1.090]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[APredicting TS 16469\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:05<00:00,  0.20it/s]\u001b[ABest score threshold values {'cytosolic-ribosome': 0.19, 'virus-like-capsid': 0.5, 'beta-galactosidase': 0.13, 'ferritin-complex': 0.16, 'beta-amylase': 0.25, 'thyroglobulin': 0.18}\n",
      "{'score_cytosolic-ribosome': 0.018348623853211007, 'score_virus-like-capsid': 0.0, 'score_beta-galactosidase': 0.01408839779005525, 'score_ferritin-complex': 0.1961067051189618, 'score_beta-amylase': 0.0, 'score_thyroglobulin': 0.02675359712230216, 'score': np.float64(0.04230561697098394)}\n",
      "\n",
      "Epoch 2: 100%|██████████| 12/12 [07:04<00:00,  0.03it/s, v_num=0, train_loss_step=0.720, val_loss_step=0.000, val_loss_epoch=0.000, val_score=0.0423, train_loss_epoch=0.767]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 12/12 [07:04<00:00,  0.03it/s, v_num=0, train_loss_step=0.720, val_loss_step=0.000, val_loss_epoch=0.000, val_score=0.0423, train_loss_epoch=0.767]\n"
     ]
    }
   ],
   "source": [
    "# code for running model training in Juputer with live printouts. You can also run the commands directly in a terminal.\n",
    "# It take about 2 min per epoch.\n",
    "# Download data can shorten the data loading overhead per epoch.\n",
    "\n",
    "from topcup.cli.cli import cli\n",
    "\n",
    "training_outputs = os.path.abspath('./outputs_training')\n",
    "\n",
    "cli.main(\n",
    "    args=[\n",
    "        \"train\",\n",
    "        \"-c\", f\"{str(copick_config_path)}\",\n",
    "        \"-u\", \"data-portal\",\n",
    "        \"-tts\", \"16463,16464,16465,16466,16467,16468\",\n",
    "        \"-vts\", \"16469\",\n",
    "        \"-n\", \"16\",  # use default value to replicate the performance\n",
    "        \"-o\", f\"{str(training_outputs)}\", \n",
    "        \"--pixelsize\", \"10.012\",\n",
    "        \"-tt\", \"wbp-denoised-denoiset-ctfdeconv\",\n",
    "        \"-v\", \"0\",\n",
    "        \"-e\", \"3\"\n",
    "    ],\n",
    "    standalone_mode=False,  # so Click doesn’t exit on exceptions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ror84XL9ejDV"
   },
   "source": [
    "## Analysis of Model Outputs\n",
    "\n",
    "The model will automatically track the validation performance and save the best checkpoint and history metrics inside the specified output directory. The evaluation score for each epoch will be shown in the printouts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enG3q8GGflGv"
   },
   "source": [
    "## Summary\n",
    "We showed how to train TopCUP model for a cryoET dataset with annotations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7sWxXkEfr7x"
   },
   "source": [
    "## Contact and Acknowledgments\n",
    "\n",
    "For issues with this notebook please contact kevin.zhao@czii.org\n",
    "\n",
    "Special thank you to Christof Hankel for developing the segmenation models and Ermel Utz for developing copick.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RIWjiHrf1ft"
   },
   "source": [
    "## References\n",
    "\n",
    "- Peck, A., et al., (2025) A Realistic Phantom Dataset for Benchmarking Cryo-ET Data Annotation. Nature Methods. DOI: 10.1101/2024.11.04.621686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0XhyjvicrVp"
   },
   "source": [
    "## Responsible Use\n",
    "We are committed to advancing the responsible development and use of artificial intelligence. Please follow our [Acceptable Use Policy](https://virtualcellmodels.cziscience.com/acceptable-use-policy) when engaging with our services."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "topcup",
   "language": "python",
   "name": "topcup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
