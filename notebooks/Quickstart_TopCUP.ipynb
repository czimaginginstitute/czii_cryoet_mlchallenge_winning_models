{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT5SZxmxZLZE"
   },
   "source": [
    "# Run TopCUP models to extract protein particels in cryoET dataset\n",
    "\n",
    "**Estimated time to complete:** 3 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbeoPlg5aCnw"
   },
   "source": [
    "## Learning Goals\n",
    "* Create a copick configuration file for loading cryoET dataset. \n",
    "* Run TopCUP model to extract particle locations via its command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb7T-ymlaOKy"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "The TopCUP model requires `python>=3.10`. Install using the command: `pip install git+https://github.com/czimaginginstitute/czii_cryoet_mlchallenge_winning_models.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqgtQsXOayO7"
   },
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "The Top CryoET U-Net Picker (TopCUP) is a 3D U-Net–based ensemble model designed for particle picking in cryo-electron tomography (cryoET) volumes.\n",
    "It uses a segmentation heatmap approach to identify particle locations.\n",
    "TopCUP is fully integrated with copick — a flexible cryoET dataset API developed at the Chan Zuckerberg Imaging Institute (CZII).\n",
    "This integration makes it easy to apply the model directly to any cryoET dataset in copick format.\n",
    "The only input required is a copick configuration file.\n",
    "\n",
    "The copick configuration file must define **pickable objects** (i.e., the protein complexes you want to detect) and **three** key metadata parameters for each object:\n",
    "* score weight: weight for each class in the DenseCrossEntropy loss\n",
    "* score_threshold: threshold to filter final picks per class, reducing false positives\n",
    "* score_weight: weight for each class in the F-beta score evaluation\n",
    "\n",
    "\n",
    "For this tutorial, we’ll use seven tomograms from the Private Test Dataset (Dataset ID: DS-10446).\n",
    "Now that this dataset is publicly available on the CZ CryoET Data Portal,\n",
    "we can stream it directly using the copick configuration file provided below.\n",
    "We can automatically generate a copick configuration file from cryoET dataportal, and add metadata for each particles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copick\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    \"ferritin-complex\": {\n",
    "        \"score_weight\": 1,\n",
    "        \"score_threshold\": 0.16,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"thyroglobulin\": {\n",
    "        \"score_weight\": 2,\n",
    "        \"score_threshold\": 0.18,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"beta-galactosidase\": {\n",
    "        \"score_weight\": 2,\n",
    "        \"score_threshold\": 0.13,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"beta-amylase\": {\n",
    "        \"score_weight\": 0,\n",
    "        \"score_threshold\": 0.25,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"cytosolic-ribosome\": {\n",
    "        \"score_weight\": 1,\n",
    "        \"score_threshold\": 0.19,\n",
    "        \"class_loss_weight\": 256\n",
    "    },\n",
    "    \"virus-like-capsid\": {\n",
    "        \"score_weight\": 1,\n",
    "        \"score_threshold\": 0.5,\n",
    "        \"class_loss_weight\": 256\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "copick_config_path = os.path.abspath('./copick_config_portal.json')\n",
    "overlay_path = os.path.abspath('./tmp_overlay')\n",
    "copick_root = copick.from_czcdp_datasets(\n",
    "    [10446], # ML Challenge private test dataset\n",
    "    overlay_path,  \n",
    "    {'auto_mkdir': True}, #overlay_root, self-defined\n",
    "    output_path = copick_config_path,\n",
    ")\n",
    "\n",
    "# only consider the 6 particles\n",
    "config_pickable_objects = []\n",
    "for p in copick_root.config.pickable_objects:\n",
    "    if p.name in metadata:\n",
    "        p.metadata = metadata[p.name]\n",
    "        config_pickable_objects.append(p)\n",
    "\n",
    "copick_root.config.pickable_objects = config_pickable_objects\n",
    "#save the copick config for later use\n",
    "copick_root.save_config(copick_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diy3C_v8bBRy"
   },
   "source": [
    "### More Resources\n",
    "\n",
    "You can find additional instructions and template configurations for accessing datasets across different platforms in the official [copick document](https://copick.github.io/copick/examples/overview/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PqqRcqDc_LG"
   },
   "source": [
    "## Run Model Inference\n",
    "\n",
    "To explore the available options for running TopCUP, use the --help flag. In your terminal, run `topcup inference --help`. This will display all command-line options and arguments for running TopCUP inference, see below:\n",
    "\n",
    "```\n",
    "Usage: topcup inference [OPTIONS]\n",
    "\n",
    "Options:\n",
    "  -c, --copick_config FILE        copick config file path  [required]\n",
    "  -ts, --run_names TEXT           Tomogram dataset run names\n",
    "  -bs, --batch_size INTEGER       batch size for data loader\n",
    "  -p, --pretrained_weights TEXT   Pretrained weights file paths (use comma for\n",
    "                                  multiple paths). Default is None.\n",
    "  -pa, --pattern TEXT             The key for pattern matching checkpoints.\n",
    "                                  Default is *.ckpt\n",
    "  --pixelsize FLOAT               Pixelsize in angstrom. Default is 10.0A.\n",
    "  -tt, --tomo_type TEXT\n",
    "                                  Tomogram type. Default is denoised.\n",
    "  -u, --user_id TEXT              Needed for training, the user_id used for\n",
    "                                  the ground truth picks.\n",
    "  -o, --output_dir TEXT           output dir for saving prediction results\n",
    "                                  (csv).\n",
    "  -g, --gpus INTEGER              Number of GPUs for inference. Default is 1.\n",
    "  -gt, --has_ground_truth BOOLEAN\n",
    "                                  Inference with ground truth annoatations\n",
    "  -h, --help                      Show this message and exit.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading topcup_phantom_6_tomograms.ckpt ...\n",
      "→ Saved to checkpoints/topcup_phantom_6_tomograms.ckpt\n",
      "Downloading topcup_phantom_12_tomograms.ckpt ...\n",
      "→ Saved to checkpoints/topcup_phantom_12_tomograms.ckpt\n",
      "Downloading topcup_phantom_24_tomograms.ckpt ...\n",
      "→ Saved to checkpoints/topcup_phantom_24_tomograms.ckpt\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "TOPCUP_CHECKPOINTS_URL = [\n",
    "    \"https://huggingface.co/kevinzhao/TopCUP/resolve/main/topcup_weights/topcup_phantom_6_tomograms.ckpt\",\n",
    "    \"https://huggingface.co/kevinzhao/TopCUP/resolve/main/topcup_weights/topcup_phantom_12_tomograms.ckpt\",\n",
    "    \"https://huggingface.co/kevinzhao/TopCUP/resolve/main/topcup_weights/topcup_phantom_24_tomograms.ckpt\",\n",
    "]\n",
    "\n",
    "# local directory to save the checkpoints\n",
    "cache = Path(\"./checkpoints\")\n",
    "cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for url in TOPCUP_CHECKPOINTS_URL:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    dest = cache / filename\n",
    "    if not dest.exists():\n",
    "        print(f\"Downloading {filename} ...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, dest)\n",
    "            print(f\"→ Saved to {dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "    else:\n",
    "        print(f\"Already exists: {dest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Options\n",
    "You can explore dataset-specific options such as `run_names`, `pixelsize`, and `tomo_type` using the copick API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run name: 17803, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 17804, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 17805, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 17806, available voxelsize/pixelsize: 4.99,10.012 A\n",
      "run name: 17807, available voxelsize/pixelsize: 4.99,10.012 A\n"
     ]
    }
   ],
   "source": [
    "import copick\n",
    "\n",
    "# Check available run names, show first 5 tomograms\n",
    "for run in copick_root.runs[:5]:\n",
    "    pss = [str(vs.voxel_size) for vs in run.voxel_spacings]\n",
    "    ps =','.join(set(pss))\n",
    "    print(f\"run name: {run.name}, available voxelsize/pixelsize: {ps} A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 17803 has tomogram_type: wbp-denoised-denoiset-ctfdeconv,wbp-filtered-ctfdeconv\n"
     ]
    }
   ],
   "source": [
    "# Get a single run\n",
    "run = copick_root.get_run('17803')\n",
    "voxel_spacing_obj = run.get_voxel_spacing(10.012)\n",
    "\n",
    "# Check available reconstruction_type\n",
    "tts = [t.tomo_type for t in voxel_spacing_obj.tomograms]\n",
    "tt = ','.join(tts)\n",
    "print(f'run {run.name} has tomogram_type: {tt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Protein Locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CaTBGVOSehLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making output dir output/inference\n",
      "[INFO] Loading 3 checkpoints from checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.2, which is newer than your current Lightning version: v2.4.0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference_dataset length: 3\n",
      "Predicting DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s][INFO] GPU cuda:0 | Using ensemble of 3 models.\n",
      "Predicting TS 17803\n",
      "Predicting DataLoader 0:  33%|███▎      | 1/3 [00:20<00:40,  0.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.czii/kevin.zhao/conda_envs/topcup/lib/python3.12/site-packages/pytorch_lightning/loops/prediction_loop.py:255: predict returned None if it was on purpose, ignore this warning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPU cuda:0 | Using ensemble of 3 models.\n",
      "Predicting TS 17804\n",
      "Predicting DataLoader 0:  67%|██████▋   | 2/3 [00:38<00:19,  0.05it/s][INFO] GPU cuda:0 | Using ensemble of 3 models.\n",
      "Predicting TS 17805\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:56<00:00,  0.05it/s]Save predicted results in output/inference/val_pred_df_seed.csv\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:56<00:00,  0.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# code for running model for inference in Juputer with live printouts. You can also run the commands directly in a terminal.\n",
    "\n",
    "from topcup.cli.cli import cli\n",
    "\n",
    "# Let's do inference for the first 3 tomograms \n",
    "cli.main(\n",
    "    args=[\n",
    "        \"inference\",\n",
    "        \"-c\", f\"{copick_config_path}\",\n",
    "        \"-ts\", \"17803,17804,17805\",\n",
    "        \"-p\", f\"{cache}\",\n",
    "        \"--pixelsize\", \"10.012\",\n",
    "        \"-o\", \"output/inference\",\n",
    "        \"-tt\", \"wbp-denoised-denoiset-ctfdeconv\",\n",
    "        \"-pa\", \"*.ckpt\",\n",
    "    ],\n",
    "    standalone_mode=False,  # so click doesn’t exit on exceptions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ror84XL9ejDV"
   },
   "source": [
    "## Model Outputs\n",
    "\n",
    "The model will automatically save the particle picks (locations in Angstrom as a csv file inside the specified output directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7sWxXkEfr7x"
   },
   "source": [
    "## Contact and Acknowledgments\n",
    "\n",
    "For issues with this quickstart please contact kevin.zhao@czii.org\n",
    "\n",
    "Special thank you to Christof Hankel for developing the segmenation models and Ermel Utz for developing copick.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RIWjiHrf1ft"
   },
   "source": [
    "## References\n",
    "\n",
    "- Peck, A., et al., (2025) A Realistic Phantom Dataset for Benchmarking Cryo-ET Data Annotation. Nature Methods. DOI: 10.1101/2024.11.04.621686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WobpA6HMY7oE"
   },
   "source": [
    "## Responsible Use\n",
    "\n",
    "We are committed to advancing the responsible development and use of artificial intelligence. Please follow our [Acceptable Use Policy](https://virtualcellmodels.cziscience.com/acceptable-use-policy) when engaging with our services."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "topcup",
   "language": "python",
   "name": "topcup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
